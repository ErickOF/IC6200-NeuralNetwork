{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph4bDLIvxm9J"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from functools import partial\n",
        "from IPython.display import clear_output\n",
        "from tqdm import trange\n",
        "from typing import Iterator, List, Tuple"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POdD19xu76X_"
      },
      "source": [
        "# Neural Network\n",
        "<img src=\"https://miro.medium.com/max/500/0*nipS8XYMKvUjvLoc.png\">\n",
        "\n",
        "## Backpropagation\n",
        "\n",
        "To compute loss gradients w.r.t input, we need to apply chain rule\n",
        "\n",
        "$\\frac{dLoss}{dx} = \\frac{dLoss}{dLayer} \\cdot \\frac{dLayer}{dx}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnLFsZF-zI9n"
      },
      "source": [
        "class Layer:\n",
        "    \"\"\"Abstract class for Layers\n",
        "    \"\"\"\n",
        "    def forward(self, x: np.array) -> np.array:\n",
        "        \"\"\"Takes input data and returns output data\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def backward(self, x: np.array, grad_output: np.array) -> np.array:\n",
        "        \"\"\"Performs a backpropagation step through the layer\n",
        "        \"\"\"\n",
        "        num_units: int = x.shape[1]\n",
        "        dlayer_dx: np.array = np.eye(num_units)\n",
        "\n",
        "        # Chain rule\n",
        "        return np.dot(grad_output, dlayer_dx)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J92e8FIw7Ui_"
      },
      "source": [
        "## [Rectified Linear Activation Function (ReLU)](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)\n",
        "\n",
        "We can describe this using a simple if-statement:\n",
        "\n",
        "```python\n",
        "if input > 0:\n",
        "\treturn input\n",
        "else:\n",
        "\treturn 0\n",
        "```\n",
        "\n",
        "We can describe this function $g()$ mathematically using the $max()$ function over the set of $0.0$ and the input $z$; for example:\n",
        "\n",
        "```\n",
        "g(z) = max{0, z}\n",
        "```\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/3228/1*LiBZo_FcnKWqoU7M3GRKbA.png\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzJvVnCazKvm"
      },
      "source": [
        "class ReLU(Layer):\n",
        "    \"\"\"Rectified Linear Activation Layer\n",
        "    \"\"\"\n",
        "    def forward(self, x: np.array) -> np.array:\n",
        "        \"\"\"Computes ReLU forward\n",
        "        \"\"\"\n",
        "        return np.maximum(0, x)\n",
        "    \n",
        "    def backward(self, x: np.array, grad_output: np.array) -> np.array:\n",
        "        \"\"\"Computes backpropagation of ReLU Layer\n",
        "        \"\"\"\n",
        "        relu_grad: np.array = x > 0\n",
        "        return grad_output * relu_grad"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG9cCKjsYlxc"
      },
      "source": [
        "## Dense Layer\n",
        "\n",
        "A dense layer applies affine transformation. In a vectorized form, it can be described as:\n",
        "\n",
        "$f(X) = W \\cdot X + \\vec{b}$\n",
        "\n",
        "Where\n",
        "* X: feature matrix\n",
        "* W: weight matrix\n",
        "* b: vector of num_outputs biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4NnFI6M0XuX"
      },
      "source": [
        "class Dense(Layer):\n",
        "    \"\"\"Regular densely-connected Neural Network layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_inputs: int, n_outputs: int,\n",
        "                 learning_rate: float = 0.001) -> None:\n",
        "        \"\"\"Dense implements the operation: output = dot(x, weights) + bias\n",
        "        weights is a weights matrix created by the layer, and bias is a bias.\n",
        "        \"\"\"\n",
        "        # Learning rate\n",
        "        self.learning_rate: float = learning_rate\n",
        "        # Weight of the layer\n",
        "        self.weights: np.array = np.random.normal(loc=0.0, \n",
        "                                                  scale=np.sqrt(2 / (n_inputs +\\\n",
        "                                                                    n_outputs)), \n",
        "                                                  size=(n_inputs, n_outputs))\n",
        "        # Bias\n",
        "        self.biases: np.array = np.zeros(n_outputs)\n",
        "\n",
        "        # Previous deltas\n",
        "        self.dweights: np.array = np.zeros((n_inputs, n_outputs))\n",
        "        # Bias\n",
        "        self.dbiases: np.array = np.zeros(n_outputs)\n",
        "\n",
        "    def forward(self, x: np.array) -> np.array:\n",
        "        \"\"\"Perform an affine transformation\n",
        "        \"\"\"\n",
        "        return np.dot(x, self.weights) + self.biases\n",
        "\n",
        "    def backward(self, x: np.array, grad_output: np.array) -> np.array:\n",
        "        \"\"\"Compute backpropagation of Dense Layer\n",
        "        \"\"\"\n",
        "        # Compute df/dx = df/ddense * ddense /dx\n",
        "        # ddense/dx = weights transposed\n",
        "        grad_x: np.array = np.dot(grad_output, self.weights.T)\n",
        "\n",
        "        # Compute gradient weights and biases\n",
        "        grad_weights: np.array = np.dot(x.T, grad_output)\n",
        "        grad_biases: np.array = grad_output.mean(axis=0) * x.shape[0]\n",
        "\n",
        "        # Regularized delta rule\n",
        "        self.dweights = self.learning_rate * (grad_weights - self.dweights)\n",
        "        self.dbiases = self.learning_rate * (grad_biases - self.dbiases)\n",
        "\n",
        "        self.weights -= self.dweights\n",
        "        self.biases -= self.dbiases\n",
        "        \n",
        "        return grad_x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1L_2T4G6uy3"
      },
      "source": [
        "def softmax_crossentropy_with_logits(logits: np.array,\n",
        "                                     reference_answers: np.array) -> np.array:\n",
        "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct\n",
        "    answers\n",
        "    \"\"\"\n",
        "    logits_for_answers: np.array = logits[np.arange(len(logits)),\n",
        "                                                    reference_answers]\n",
        "\n",
        "    xentropy: np.array = -logits_for_answers + np.log(np.sum(np.exp(logits),\n",
        "                                                             axis=-1))\n",
        "\n",
        "    return xentropy\n",
        "\n",
        "def grad_softmax_crossentropy_with_logits(logits: np.array,\n",
        "                                          reference_answers: np.array) -> np.array:\n",
        "    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of\n",
        "    correct answers\"\"\"\n",
        "    ones_for_answers: np.array = np.zeros_like(logits)\n",
        "    ones_for_answers[np.arange(len(logits)), reference_answers] = 1\n",
        "\n",
        "    softmax: np.array = np.exp(logits) / np.exp(logits).sum(axis=-1,\n",
        "                                                            keepdims=True)\n",
        "\n",
        "    return (-ones_for_answers + softmax) / logits.shape[0]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5bCxMDI0ckk"
      },
      "source": [
        "def generate_dataset(size: tuple = (10, 10), n_samples: int = 1000,\n",
        "                     flatten: bool = True) -> Tuple[np.array]:\n",
        "    # Generate samples\n",
        "    samples: np.array = np.random.randint(0, 2, (n_samples,) + size) / 1.0\n",
        "\n",
        "    # Generate labels\n",
        "    non_zeros: np.array = np.count_nonzero(samples, axis=(1, 2))\n",
        "    labels: np.arry = np.zeros((n_samples,), dtype=np.uint8)\n",
        "    labels[non_zeros > (size[0] * size[1] // 2)] = 1\n",
        "\n",
        "    if flatten:\n",
        "        samples = samples.reshape([samples.shape[0], -1])\n",
        "\n",
        "    return samples, labels\n",
        "\n",
        "X_train, y_train = generate_dataset((10, 10), 800000)\n",
        "X_val, y_val = generate_dataset((10, 10), 2000000)\n",
        "X_test, y_test = generate_dataset((10, 10), 1000000)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWsbS-V60iX9"
      },
      "source": [
        "class Network:\n",
        "    \"\"\"Neural Network\n",
        "    \"\"\"\n",
        "    def __init__(self, layers: List[Layer]):\n",
        "        \"\"\"Constructor\n",
        "        \"\"\"\n",
        "        self.network: List[Layer] = layers\n",
        "\n",
        "    def forward(self, X: np.array) -> List[np.array]:\n",
        "        \"\"\"Compute activations of all network layers by applying them\n",
        "        sequentially. Return a list of activations for each layer.\n",
        "        \"\"\"\n",
        "        activations: List[np.array] = []\n",
        "        x: np.array = X\n",
        "\n",
        "        # Looping through each layer\n",
        "        for layer in self.network:\n",
        "            activations.append(layer.forward(x))\n",
        "            # Updating input to last layer output\n",
        "            x: np.array = activations[-1]\n",
        "\n",
        "        return activations\n",
        "\n",
        "    def load(self, file_name: str = '0') -> None:\n",
        "        \"\"\"Load network weights\n",
        "        \"\"\"\n",
        "        # Modify allow_pickle parameter\n",
        "        np_load_old = partial(np.load)\n",
        "        np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
        "\n",
        "        # Load weights\n",
        "        loaded_weights: np.array = np.load(f'{file_name}_nnw.npy')\n",
        "        # Index of the current weight\n",
        "        i: int = 0\n",
        "\n",
        "        for layer in self.network:\n",
        "            if isinstance(layer, Dense):\n",
        "                # Load weight into the layer\n",
        "                layer.weights = loaded_weights[i]\n",
        "                i += 1\n",
        "\n",
        "    def predict(self, X: np.array) -> List[np.array]:\n",
        "        \"\"\"Compute network predictions. Returning indices of largest Logit\n",
        "        probability\n",
        "        \"\"\" \n",
        "        return self.forward(X)[-1].argmax(axis=-1)\n",
        "\n",
        "    def save(self, file_name: str = '0') -> None:\n",
        "        \"\"\"Save network wieghts\n",
        "        \"\"\"\n",
        "        np.save(f'{file_name}_nnw.npy',\n",
        "                np.array([l.weights for l in self.network if isinstance(l,\n",
        "                                                                    Dense)]))\n",
        "\n",
        "    def train(self, X: np.array, y: np.array) -> np.array:\n",
        "        \"\"\"Train neural network\n",
        "        \"\"\"\n",
        "        # Get the layer activations\n",
        "        layer_activations: List[np.array] = self.forward(X)\n",
        "        layer_inputs: List[List] = [X] + layer_activations\n",
        "        logits: np.array = layer_activations[-1]\n",
        "\n",
        "        # Compute the loss and the initial gradient\n",
        "        loss: np.array = softmax_crossentropy_with_logits(logits, y)\n",
        "        loss_grad: np.array = grad_softmax_crossentropy_with_logits(logits, y)\n",
        "\n",
        "        # Propagate gradients through the network\n",
        "        for layer_index in range(len(self.network))[::-1]:\n",
        "            layer: Layer = self.network[layer_index]\n",
        "            loss_grad = layer.backward(layer_inputs[layer_index], loss_grad)\n",
        "            \n",
        "        return np.mean(loss)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "A60lPtqs0nZv",
        "outputId": "ec975203-9604-416e-e959-9baf8338fde0"
      },
      "source": [
        "def iterate_minibatches(x: np.array, y: np.array,\n",
        "                        batchsize: int) -> Iterator[Tuple[np.array]]:\n",
        "    \"\"\"Generate randomly mini bataches\n",
        "    \"\"\"\n",
        "    # Randomize samples\n",
        "    index: np.array = np.random.permutation(len(x))\n",
        "\n",
        "    for start_idx in trange(0, len(x) - batchsize + 1, batchsize):\n",
        "        extract: np.array = index[start_idx:start_idx + batchsize]\n",
        "\n",
        "        yield x[extract], y[extract]\n",
        "\n",
        "EPOCHS: int = 15\n",
        "\n",
        "training_history: List = []\n",
        "validation_history: List = []\n",
        "\n",
        "layers: List[Layer] = [Dense(X_train.shape[1], 100), # Input layer\n",
        "                       ReLU(),                       # ReLU layer\n",
        "                       Dense(100, 50),               # Hidden layer\n",
        "                       ReLU(),                       # ReLU layer\n",
        "                       Dense(50, 2)]                 # Output layer\n",
        "\n",
        "nn: Network = Network(layers)\n",
        "nn.load()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for x_batch, y_batch in iterate_minibatches(X_train, y_train, batchsize=32):\n",
        "        nn.train(x_batch, y_batch)\n",
        "\n",
        "    training_history.append(np.mean(nn.predict(X_train) == y_train))\n",
        "    validation_history.append(np.mean(nn.predict(X_val) == y_val))\n",
        "\n",
        "    clear_output()\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}')\n",
        "    print(f'Train accuracy: {training_history[-1]}')\n",
        "    print(f'Validation accuracy: {validation_history[-1]}')\n",
        "\n",
        "    plt.plot(training_history,label='Train accuracy')\n",
        "    plt.plot(validation_history,label='Valation accuracy')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    if validation_history[-1] > 0.95:\n",
        "        break\n",
        "\n",
        "nn.save()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train accuracy: 0.96435625\n",
            "Validation accuracy: 0.9641855\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}